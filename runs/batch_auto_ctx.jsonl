{"question": "Est-ce que ce qui est mis en valeur est le fond noir ou le contour de la plante ?", "answer": "", "context": "Sur la classe 'Corn_Healthy', on peut avoir un doute sur l'angle gauche de l'image qui est coloré : Est-ce que ce qui est mis en valeur est le fond noir ou le contour de la plante ? Cette surbrillance n'étant pas mise en valeur sur le coin bas à gauche, je pense qu'il s'agit bien du contour de la plante, permettant sa reconnaissance. La colonne SHAP contient la classe pour laquelle l'image contient le plus de caractéristiques. Il confirme que notre modèle est capable de capturer les motifs et les éléments visuels pertinents pour effectuer des prédictions précises, étant donné qu'il s'agit à chaque fois de la bonne classe d'image sauf pour la classe 49 (Mosaic Virus). En analysant les graphiques sur cette classe, on confirme que le modèle a du mal à tirer ses caractéristiques spécifiques avec la classe 46.\n\nPlusieurs traitements sont appliqués aux images dans le but d'améliorer leur qualité. Les traitements incluent des ajustements sur la luminosité, le contraste, la netteté (flou et netteté), ainsi que des redimensionnements. Les étapes sont détaillées ci-dessous :\n\nDans certaines images, des zones d'attention sont visibles en dehors des feuilles, notamment sur l'arrière-plan ou sur d'autres objets. Cela suggère que le modèle peut être distrait par des éléments non pertinents.\n\n- Non exploité car bases de données non spécifiquement axées sur les plantes, de volumétrie limitée sur cette dernière catégorie, sans discrimination entre plantes saines et plantes malades, dont le seul intérêt était la présence de masques qui ne serviront pas dans les algorithmes choisis.\n\n- Conseil agricole : Outil d'aide au diagnostic pour conseillers et techniciens agricoles - Formation : Support pédagogique pour l'apprentissage de la reconnaissance des maladies - Certification : Contrôle qualité automatisé pour les filières agricoles\n\nL'application sera développée sur Streamlit conformément au schéma de l'application présentée en Page 6 en veillant à des règles d'ergonomie de « base » (éviter les images de fond, contraste des couleurs pour faciliter la lecture, usage des couleurs limité à une gamme réduite, boutons de navigation explicites, etc.) Jalons du projet\n\nLocaliser et classifier la maladie éventuelle touchant la plante dans cette même image La photographie retraitée, ou la photographie d'origine subissant de nouveaux filtres, sera soumise à un second algorithme de classification, entraîné sur la base totale des images décrite en page 14 et de méta-données pouvant y être rattachées (telles que le niveau de bleu dans l'image qui est un premier prédicteur du caractère malade ou non de la plante) et testé sur une base constituée de 20% des images concernées avec un objectif d'accuracy de 95% des feuilles malades. Bâtir une application renvoyant les informations de ces classifications à l'utilisateur sur la base d'une photographie prise sur son smartphone.\n\nLa fonction `calculate_blurriness(image)` utilise la méthode du Laplacien pour calculer le flou d'une image. Le flou est mesuré par la variance de la matrice du Laplacien sur l'image en niveaux de gris. Une faible variance indique un flou plus élevé, tandis qu'une haute variance indique une image plus nette.", "answer_error": ""}
{"question": "Quels modèles ont été testés dans le projet ?", "answer": "", "context": "- Choix architecturaux : Nécessité de réviser l'approche initiale face aux performances insuffisantes sur certaines classes, menant à l'exploration de modèles alternatifs (SwinTransformer) ou l'utilisation de certaines images pré-retraitement (Cassave) - Métriques d'évaluation : Remise en question de l'accuracy comme métrique principale face au déséquilibre des classes et utilisation du F1-score - Stratégie de données : Questionnement sur l'inclusion/exclusion de la classe Cassave dans le modèle final - Puissance computationnelle : Temps d'entraînement prohibitifs pour certains modèles (EfficientNetV2M, VGG16) nécessitant l'optimisation du code et la réduction de la taille des images - Mémoire : Contraintes mémoire lors de l'entraînement de modèles profonds avec des images haute résolution - Déploiement : Défis de compression des modèles pour un usage mobile tout en préservant les performances\n\nNous avons exploré deux approches principales pour combiner les modèles : Soft Voting (vote pondéré par les probabilités) : chaque modèle contribue à la prédiction finale via sa distribution de probabilité. Nous avons initialement utilisé une pondération équivalente pour chaque modèle. Une étude systématique des coefficients de pondération n'a pas mis en évidence de combinaison significativement meilleure que l'équipondération. Cette méthode s'est avérée simple, stable, et a permis d'obtenir des résultats compétitifs. Stacking : cette approche consistait à utiliser un méta-modèle entraîné à partir des sorties des modèles de base.\n\nSelon les modèles et afin de nous assurer, malgré le faible déséquilibre de classes, que les classes les moins représentées soient correctement traitées, nous avons observé dans certains cas quelle était la k-ème Accuracy la plus faible parmi les classes proposées au modèle.\n\nL'application sera développée sur Streamlit conformément au schéma de l'application présentée en Page 6 en veillant à des règles d'ergonomie de « base » (éviter les images de fond, contraste des couleurs pour faciliter la lecture, usage des couleurs limité à une gamme réduite, boutons de navigation explicites, etc.) Jalons du projet\n\n- 1 modèle simple sans Transfer Learning, comprenant une première couche de convolution analysant le couleurs RGB (3 canaux) avec 32 filtres détectant les motifs simples et les contours, un deuxième bloc convolutif avec 64 filtres permettant de capter des motifs plus complexes (formes des feuilles, textures), une couche dense interprétant les motifs détectés et enfin la dernière couche de sortie donnant les probabilités par classe de plantes. La représentation du modèle est portée en Annexes. - Les 2 modèles ont été testés successivement sur 18 classes (espèces) et 58 classes (croisements espèces x maladies)\n\nLa deuxième phase a été consacrée à l'interprétation des modèles en utilisant des outils comme GradCam ou SHAP ainsi qu'à l'apprentissage, par lequel nous commençons cette phase, d'un modèle alternatif aux CNN et Transformer Standard : Swintransformer.\n\nLes résultats suivants comparent les performances des modèles avant et après application des trois stratégies (filtrage, relabellisation, augmentation) sur l'ensemble des classes (global) et spécifiquement sur les classes Cassava.\n\n- Le modèle entraîné sous Pytorch sans transfer learning, en raison de son taux de précision sur les images tests de 83%, très inférieur aux modèles pré-entraînés testés dans le chapitre précédent. - Le modèle AlexNet, qui affiche à la fois un temps d'entraînement trop important et un taux de précision de 88%, également très en-deça des autres modèles testés.", "answer_error": ""}
{"question": "Quelles métriques principales et secondaires ont été utilisées ?", "answer": "", "context": "La tâche associée est la reconnaissance d'images , spécifiquement de feuilles associées à 18 espèces végétales (fruits, légumineuses, céréales), dont les contours puis les détails sont progressivement analysées à travers les couches du réseau de neurones modélisé. Métriques de performance utilisées pour comparer les modèles A ce stade nous avons choisi 2 métriques. Métrique principale Accuracy : Compte tenu de la limitation du déséquilibre entre classes obtenu après la phase de pre-processing, nous avons choisi de retenir l'Accuracy comme métrique principale, définie comme le rapport entre le nombre de bonnes prédictions et le nombre total de prédictions. Métriques secondaires Loss : Nous avons également jaugé la rapidité de la convergence du modèle en suivant la notion de loss à chaque époque de l'apprentissage.\n\nPlusieurs traitements sont appliqués aux images dans le but d'améliorer leur qualité. Les traitements incluent des ajustements sur la luminosité, le contraste, la netteté (flou et netteté), ainsi que des redimensionnements. Les étapes sont détaillées ci-dessous :\n\n- Les classes minoritaires (Healthy, Bacterial blight, Brown streak disease, Green mottle) ont été massivement augmentées. - L'objectif était d'atteindre un volume comparable à la classe majoritaire (Cassava Mosaic Disease). - Les techniques utilisées incluent : rotations, flips, changements de luminosité, contrastes, translations, etc.\n\nL'application sera développée sur Streamlit conformément au schéma de l'application présentée en Page 6 en veillant à des règles d'ergonomie de « base » (éviter les images de fond, contraste des couleurs pour faciliter la lecture, usage des couleurs limité à une gamme réduite, boutons de navigation explicites, etc.) Jalons du projet\n\nLa deuxième phase a été consacrée à l'interprétation des modèles en utilisant des outils comme GradCam ou SHAP ainsi qu'à l'apprentissage, par lequel nous commençons cette phase, d'un modèle alternatif aux CNN et Transformer Standard : Swintransformer.\n\nNous n'avons pas eu recours à des experts externes à proprement parler (en-dehors de Damien évidemment !) mais avons réalisé de nombreuses recherches afin de nous guider, notamment sur les sujets suivants : - Taille des images standard utilisée dans les algorithmes de reconnaissance d'image ; - Détermination du niveau de flou d'une image et seuil d'acceptabilité pour la reconnaissance d'images (150 de Laplacienne) ; - Retraitement des niveaux de flous ; - Déséquilibre acceptable entre classes sur le fichier d'entraînement (1 à 10) ; Nombre minimal d'images à soumettre à l'algorithme par classe (1.000).\n\n, Résultats Global rank = . , models = EffNetV2M Swin. , accuracy = 0.9865. , fl score = 0.9856. , nb_models = . , cassava_only = False. , Résultats Global rank = . , models = EffNetV2M ResNet5oV2. , accuracy = 0.9855. , fl score = 0.9847. , nb_models = . , cassava_only = False. , Résultats Global rank = . , models = EffNetV2M. , accuracy = 0.9858. , fl score = 0.9845. , nb_models = . , cassava_only = False. , Résultats Global rank = . , models = EffNetV2M. , accuracy = 0.9841. , fl score = 0.9837. , nb_models = . , cassava_only = False. , Résultats Global rank = . , models = EffNetV2M Swin. , accuracy = 9851. , fl score = 9834. , nb_models = . , cassava_only = False. , Résultats\n\nWidth Width Avant retraitement, les images avaient des dimensions très variées, ce qui compliquait leur analyse et leur traitement uniforme. Cette hétérogénéité des tailles pouvait entraîner des biais lors de l'application de modèles d'apprentissage automatique ou d'autres analyses d'images, car les réseaux de neurones et les algorithmes de traitement d'images nécessitent des entrées de taille uniforme. Après retraitement, toutes les images ont été redimensionnées à une taille standard de 256x256 pixels, un format couramment utilisé dans la datascience. Cette normalisation permet non seulement de faciliter l'analyse et l'entraînement des modèles, mais elle garantit également que toutes les images sont traitées de manière cohérente, améliorant ainsi la performance des modèles d'analyse et de classification.", "answer_error": ""}
{"question": "Quels sont les objectifs du projet et le contexte d'usage ?", "answer": "", "context": "La cible du projet est d' offrir à un individu, ne bénéficiant pas de l'expertise pour reconnaître une plante et sa maladie éventuelle, la capacité de l'identifier via l'appareil photo de son smartphone - Du point de vue technique, il s'agit d'utiliser des algorithmes de machine learning, pour analyser les images et classifier les espèces de plantes, détecter les maladies et réaliser des analyses statistiques afin de déterminer, le cas échéant, les corrélations entre espèces de plantes et maladies associées. - Du point de vue économique, l'application construite pourra aider un certain nombre d'acteurs (agriculteurs, distributeurs de plantes, paysagiste, …) à détecter précocement les maladies et éviter des pertes de récoltes ou de chiffre d'affaires, et minimiser les coûts associés aux traitements tardifs.\n\nLes résultats suivants comparent les performances des modèles avant et après application des trois stratégies (filtrage, relabellisation, augmentation) sur l'ensemble des classes (global) et spécifiquement sur les classes Cassava.\n\nL'application sera développée sur Streamlit conformément au schéma de l'application présentée en Page 6 en veillant à des règles d'ergonomie de « base » (éviter les images de fond, contraste des couleurs pour faciliter la lecture, usage des couleurs limité à une gamme réduite, boutons de navigation explicites, etc.) Jalons du projet\n\nLe modèle ne focalise pas systématiquement sur les mêmes zones d'une image à l'autre, ce qui reflète à la fois une adaptation aux différences d'images et un possible manque de stabilité dans la reconnaissance.\n\n- Choix architecturaux : Nécessité de réviser l'approche initiale face aux performances insuffisantes sur certaines classes, menant à l'exploration de modèles alternatifs (SwinTransformer) ou l'utilisation de certaines images pré-retraitement (Cassave) - Métriques d'évaluation : Remise en question de l'accuracy comme métrique principale face au déséquilibre des classes et utilisation du F1-score - Stratégie de données : Questionnement sur l'inclusion/exclusion de la classe Cassave dans le modèle final - Puissance computationnelle : Temps d'entraînement prohibitifs pour certains modèles (EfficientNetV2M, VGG16) nécessitant l'optimisation du code et la réduction de la taille des images - Mémoire : Contraintes mémoire lors de l'entraînement de modèles profonds avec des images haute résolution - Déploiement : Défis de compression des modèles pour un usage mobile tout en préservant les performances\n\nEla CIMEN, Algorithmie = 2/5. Ela CIMEN, Dév. Appli mobile = 0/5. Ela CIMEN, Statistiques = 3/5. Ela CIMEN, Gestion projet = 4/5. Ela CIMEN, Botanique = 3/5. Roland KONAN, Algorithmie = 4/5. Roland KONAN, Dév. Appli mobile = 2/5. Roland KONAN, Statistiques = 3/5. Roland KONAN, Gestion projet = 4/5. Roland KONAN, Botanique = 2/5. Yacine MADI SAID, Algorithmie = 3/5. Yacine MADI SAID, Dév. Appli mobile = 5/5. Yacine MADI SAID, Statistiques = 2/5. Yacine MADI SAID, Gestion projet = 4/5. Yacine MADI SAID, Botanique = 1/5. Nicolas RINCÉ, Algorithmie = 2/5. Nicolas RINCÉ, Dév. Appli mobile = 0/5. Nicolas RINCÉ, Statistiques = 4/5. Nicolas RINCÉ, Gestion\n\n- Extension du dataset : I ntégration de bases de données supplémentaires pour couvrir plus d'espèces (objectif : passer de 19 à 100+ espèces) - Augmentation ciblée : Techniques d'augmentation spécialisées par type de maladie (GANs, style transfer) - Données multi-modales : Intégration d'informations contextuelles (géolocalisation, saison, …)\n\nLes facteurs de traitement sont définis avant le traitement : - `brightness_factor` : Ajuste la luminosité (ici fixé à 0.8 pour une légère réduction de la luminosité). - `contrast_factor` : Augmente le contraste (ici fixé à 1.5 pour un contraste plus élevé). - `sharpness_factor` : Améliore la netteté (ici fixé à 4.0 pour une forte amélioration de la netteté). - Taille de redimensionnement : Les images sont redimensionnées à 256x256 pixels pour une normalisation de la taille.", "answer_error": ""}
{"question": "Quelle est la performance globale atteinte par EfficientNetV2M ?", "answer": "", "context": "EfficientNetV2M reste central dans tous les meilleurs ensembles : c'est le modèle le plus constant et performant, en global comme sur Cassava. ResNet50V2, bien que moins performant seul, apporte une complémentarité utile en ensemble, notamment sur les classes Cassava. Le stacking n'a pas permis d'amélioration notable par rapport au soft voting, tout en nécessitant une mise en œuvre plus complexe. Il a donc été écarté dans les configurations finales. En tenant compte des contraintes pratiques de mise en œuvre de notre application (réduction des temps d'entraînement, coûts d'hébergement, vitesse d'inférence) nous avons décidé de sélectionner l'ensemble EfficientNetV2M + ResNet50V2.\n\nEfficientNetV2M, Stratégie = Base Jine. EfficientNetV2M, Accuracy Globale = 0.9858. EfficientNetV2M, Fl Score Global = 0.9845. EfficientNetV2M, Gain Accuracy = . EfficientNetV2M, Gain Fl Score = . , Stratégie = Augmentation. , Accuracy Globale = 0.9748. , Fl Score Global = 0.9773. , Gain Accuracy = 1.109. , Gain Fl Score = . , Stratégie = Filtrage. , Accuracy Globale = 0.9695. , Fl Score Global = 0.9773. , Gain Accuracy = 1.63%. , Gain Fl Score = -0.72%. , Stratégie = Relabeling. , Accuracy Globale = 0.9625. , Fl Score Global = 0.9731. , Gain Accuracy = -2.33%. , Gain Fl Score = 1.149. ResNetSOV2, Stratégie = Base Jine. ResNetSOV2, Accuracy Globale = 0.9578. ResNetSOV2,\n\nNotre contribution principale réside dans le développement d'une pipeline complète de traitement et de classification d'images de plantes intégrant : 1. Système de tri et amélioration automatique de la qualité : Développement d'algorithmes de détection et correction du flou, luminosité et contraste 2. Architecture hybride optimisée : Comparaison systématique de 6 architectures différentes avec fine-tuning spécialisé 3. Solution de déploiement adaptative : Proposition de deux modèles complémentaires (VGG12 léger pour mobile, EfficientNetV2M puissant pour cloud)\n\n- Raisins, cerises, agrumes, pommes, pêches, etc. (classes équilibrées) : les trois modèles ont des scores F1 proches de 1.00 sur toutes ces classes : aucune distinction significative. - Sugarcane (classes mineures) : Bonne performance des trois modèles. EfficientNetV2M excellent suivi de ViT16 - Tomato (13 sous-classes, très représenté mais varié) : Bonne performance des trois modèles. EfficientNetV2M excellent suivi de ViT16 En conclusion, suivant les critères liant métriques de précision, de temps d'apprentissage et de taille du modèle, EfficientNetV2M semble devoir être privilégié.\n\nFastai est une librairie open-source de haut niveau pour le deep learning. Construite sur PyTorch, elle a pour objectif de simplifier les entraînements de modèles, accélérer les déploiements de modèles tout en conservant la performance et les fonctionnalités avancées de Pytorch. Le modèle sélectionné pour l'entraînement est efficientnet_b0 . Proposé par Google, il est le plus petit des modèles de la famille EfficientNet, ce qui le rend pertinent pour son utilisation sur du matériel avec des ressources limitées (smartphone, tablette). En analysant les résultats de performance à chaque itération, on constate que :\n\n- Architecture : EfficientNet utilise une méthode d'optimisation appelée \"compound scaling\", qui ajuste simultanément la profondeur, la largeur et la résolution de l'image, offrant ainsi une meilleure efficacité énergétique et une précision supérieure. - Profundité : Moins profond que ResNet50V2, mais beaucoup plus efficace pour les mêmes performances. - Performance : Généralement plus rapide et plus efficace que ResNet50V2 pour la même précision. - Avantages : Très efficace en termes de taille du modèle et d'utilisation des ressources, il donne de bons résultats même avec des réseaux moins profonds. - Inconvénients : Moins éprouvé que ResNet50V2 dans certaines configurations.\n\nLa troisième étape s'est avérée la plus significative en matière de gain de performance, avec un traitement spécifique apportée à la classe 'Cassava' en augmentant manuellement les images des classes minoritaires et en appliquant une augmentation algorithmique ciblée. Les résultats du fine-tuning par modèle sont résumés dans le tableau ci-dessous, les deux modèles EfficientNetV2M et ViT16 se distinguant sur les classes déséquilibrées comme la Cassave.\n\nLes modèles ResNet50V2 et Swin Transformer présentent la plus forte divergence (0.081), confirmant leur grande diversité de prédiction. EfficientNetV2M et Swin présentent également une divergence modérée (0.033), suggérant une complémentarité exploitable. À l'inverse, EfficientNetV2M et ConvNeXt sont plus proches (0.038), ce qui indique une certaine redondance dans leurs décisions. Ces résultats renforcent l'hypothèse selon laquelle un ensemblage de modèles hétérogènes (ResNet + Swin + EfficientNet) peut améliorer la robustesse des prédictions globales grâce à une diversité suffisante dans les sorties.", "answer_error": ""}
{"question": "Quelles difficultés spécifiques ont été rencontrées sur la classe Cassava ?", "answer": "", "context": "Le principal verrou scientifique de ce projet s'est révélé être la qualité et la labellisation hétérogène des données d'images, particulièrement pour la classe Cassave, qui a systématiquement montré des performances de classification inférieures malgré les efforts de fine-tuning. Cette difficulté reflète un défi majeur en reconnaissance d'images agricoles : distinguer les symptômes subtils de maladies sur des images prises dans des conditions naturelles variables, avec des arrière-plans complexes et des variations d'éclairage . Difficultés détaillées par catégorie\n\nLes résultats suivants comparent les performances des modèles avant et après application des trois stratégies (filtrage, relabellisation, augmentation) sur l'ensemble des classes (global) et spécifiquement sur les classes Cassava.\n\n- L'entraînement a été réalisé avec un mécanisme d'early stopping qui interrompt l'apprentissage si la performance sur le jeu de validation ne s'améliore plus pendant trois époques consécutives. - L'optimiseur utilisé est AdamW, qui semble reconnu pour sa robustesse et son efficacité pour ce modèle avec un learning rate de 0,0001. - Des poids de classe sont calculés en fonction de la fréquence de chaque catégorie dans le jeu d'entraînement. Le poids de la classe « Cassava_mosaic_disease » est volontairement réduit afin de limiter son influence lors de l'apprentissage. Il y a en effet 5 fois plus d'images de Cassaves atteintes de cette maladie que dans les autres classes de Cassaves. - À chaque époque, la fonction de coût utilisée est la CrossEntropyLoss pondérée par ces poids de classe.\n\nCompte tenu du faible % de prédiction, la Cassave a fait l'objet d'un focus particulier. On constate que le finetuning réalisé a eu un effet spectaculaire sur la reconnaissance de ces classes (hausse du F1-score de 15pts environ chez EfficientNetV2M et ViT16) notamment sur la Cassave saine. S'agissant des autres classes, on relève les principaux enseignements suivants:\n\n- Apporter de la diversité visuelle aux classes sous-représentées. - Réduire le biais induit par la classe majoritaire. - Améliorer la généralisation du modèle sur les classes rares. 2. Filtrage basé sur un score multi-critères Une seconde approche a consisté à filtrer les images Cassava jugées peu fiables à l'aide d'un score multi-critères.\n\n- La qualité de la précision semble souffrir de la classe Cassave : quel que soit le modèle employé, et malgré les efforts de fine-tuning, il semble qu'une difficulté de reconnaissance subsiste pour cette classe. Cette difficulté est selon intrinsèquement lié à la qualité des images et aux problèmes apparents de labellisation. Nous nous interrogeons donc sur l'abandon de cette classe dans le modèle final. - Les modèles affichent des tailles variables, certains sans doute incompatibles avec un usage mobile. En conséquence, nous décidons de retenir à ce stade deux modèles, l'un léger, embarqué sur mobile (VGG12), et l'autre plus puissant hébergé dans un cloud (EfficientNetV2M).\n\nUne attention particulière sera portée sur les classes de cassave pour lesquelles le modèle obtient les moins bons résultats. Cela permettra d'identifier les causes des erreurs et d'améliorer la robustesse du modèle.\n\nLa première phase a reposé sur : - des corrections d'erreur : mesures de précision sur des images test et non les images d'entraînement ; - des 'élagages' rapides de modèles jugés non pertinents ; - l'utilisation de techniques de fine-tuning : choix des 'optimizers', gel/dégel de couches des modèles pour l'apprentissage, technique d'augmentation de data, ajustement des learning rates et introduction de scheduler, modification des fonctions de 'loss', techniques de callback, application de poids aux classes pour corriger les déséquilibres ; - des ajustements techniques pour un apprentissage plus rapide : réorganisation du code ; - des choix réalisés pour le traitement de classes dont la prédiction est plus complexe : il s'agit principalement de la classe 'Cassava' avec la suppression pure et simple de ces classes ou bien focus sur la précision augmentée des modèles au fur et à mesure du fine-tuning.", "answer_error": ""}
{"question": "Quelles stratégies de traitement des images Cassava ont été évaluées ?", "answer": "", "context": "Au vu des performances modestes obtenues par nos modèles sur le sous-ensemble Cassava, nous avons décidé de mettre en œuvre plusieurs stratégies d'amélioration. Ces approches visent à renforcer la qualité des données et à améliorer la capacité des modèles à discriminer les classes Cassava, souvent déséquilibrées et visuellement similaires.\n\nLe principal verrou scientifique de ce projet s'est révélé être la qualité et la labellisation hétérogène des données d'images, particulièrement pour la classe Cassave, qui a systématiquement montré des performances de classification inférieures malgré les efforts de fine-tuning. Cette difficulté reflète un défi majeur en reconnaissance d'images agricoles : distinguer les symptômes subtils de maladies sur des images prises dans des conditions naturelles variables, avec des arrière-plans complexes et des variations d'éclairage . Difficultés détaillées par catégorie\n\nPlusieurs traitements sont appliqués aux images dans le but d'améliorer leur qualité. Les traitements incluent des ajustements sur la luminosité, le contraste, la netteté (flou et netteté), ainsi que des redimensionnements. Les étapes sont détaillées ci-dessous :\n\n- Choix architecturaux : Nécessité de réviser l'approche initiale face aux performances insuffisantes sur certaines classes, menant à l'exploration de modèles alternatifs (SwinTransformer) ou l'utilisation de certaines images pré-retraitement (Cassave) - Métriques d'évaluation : Remise en question de l'accuracy comme métrique principale face au déséquilibre des classes et utilisation du F1-score - Stratégie de données : Questionnement sur l'inclusion/exclusion de la classe Cassave dans le modèle final - Puissance computationnelle : Temps d'entraînement prohibitifs pour certains modèles (EfficientNetV2M, VGG16) nécessitant l'optimisation du code et la réduction de la taille des images - Mémoire : Contraintes mémoire lors de l'entraînement de modèles profonds avec des images haute résolution - Déploiement : Défis de compression des modèles pour un usage mobile tout en préservant les performances\n\nUne fois les images traitées, le DataFrame Low_quality_processed contenant les informations des images traitées est enregistré dans un fichier CSV. Ce fichier peut être utilisé pour des analyses ultérieures ou pour vérifier les modifications apportées à chaque image.\n\nCassava_green mottle, 4 = 279. Cassava nosaic disease, 1 = 91. Cassava nosaic disease, 2 = 0 89. Cassava nosaic disease, 3 = 0 .90. Cassava nosaic disease, 4 = 1540 Nous avons donc tâché de comprendre pourquoi le modèle classifie mal en regardant plus en détail les images de Cassave. Nous avons choisi d'utiliser la méthode de saillance native à TensorFlow, car elle est pleinement compatible avec notre environnement et ne présente pas de problèmes d'incompatibilité, ce qui facilite son intégration.\n\nLa troisième étape s'est avérée la plus significative en matière de gain de performance, avec un traitement spécifique apportée à la classe 'Cassava' en augmentant manuellement les images des classes minoritaires et en appliquant une augmentation algorithmique ciblée. Les résultats du fine-tuning par modèle sont résumés dans le tableau ci-dessous, les deux modèles EfficientNetV2M et ViT16 se distinguant sur les classes déséquilibrées comme la Cassave.\n\n- Apporter de la diversité visuelle aux classes sous-représentées. - Réduire le biais induit par la classe majoritaire. - Améliorer la généralisation du modèle sur les classes rares. 2. Filtrage basé sur un score multi-critères Une seconde approche a consisté à filtrer les images Cassava jugées peu fiables à l'aide d'un score multi-critères.", "answer_error": ""}
{"question": "Quel a été l'impact du soft voting par rapport aux modèles individuels ?", "answer": "", "context": "Nous avons exploré deux approches principales pour combiner les modèles : Soft Voting (vote pondéré par les probabilités) : chaque modèle contribue à la prédiction finale via sa distribution de probabilité. Nous avons initialement utilisé une pondération équivalente pour chaque modèle. Une étude systématique des coefficients de pondération n'a pas mis en évidence de combinaison significativement meilleure que l'équipondération. Cette méthode s'est avérée simple, stable, et a permis d'obtenir des résultats compétitifs. Stacking : cette approche consistait à utiliser un méta-modèle entraîné à partir des sorties des modèles de base.\n\nDans cette partie, nous étudions l'impact potentiel de l'ensemblage de modèles sur l'amélioration des performances de classification. Avant d'examiner les méthodes d'ensemblage, il est important de rappeler les performances des modèles pris individuellement dans leur configuration de base (sans stratégie d'amélioration) : - -EfficientNetV2M est le modèle ayant obtenu les meilleurs résultats globaux et sur Cassava. - -Swin Transformer a montré de bonnes performances, notamment une meilleure robustesse sur les classes minoritaires. - -ConvNext était le troisième meilleur modèle. - -ResNet50V2 , bien qu'un peu moins performant, a été retenu pour sa complémentarité potentielle avec les autres modèles. Ces performances individuelles servent de point de comparaison pour évaluer les gains apportés par l'ensemblage.\n\n- Les images du premier décile de score ont été re-labellisées automatiquement. - Le nouveau label correspond à la classe prédite par le soft voting, considérée comme plus fiable que le label d'origine. - Les autres images ont conservé leur annotation initiale. Objectifs - Corriger les labels potentiellement erronés. - Maintenir la taille du dataset en remplaçant les étiquettes douteuses. - Exploiter la confiance collective des modèles pour guider la correction.\n\nSelon les modèles et afin de nous assurer, malgré le faible déséquilibre de classes, que les classes les moins représentées soient correctement traitées, nous avons observé dans certains cas quelle était la k-ème Accuracy la plus faible parmi les classes proposées au modèle.\n\nLe score est défini comme une combinaison pondérée de trois composantes : - Précision locale du soft voting : probabilité que le soft voting des modèles prédit correctement cette image sur le dataset d'entraînement. - Confiance du soft voting : probabilité moyenne attribuée à la classe majoritaire par le soft voting. - Accord inter-modèles : mesuré par la divergence de Kullback-Leibler (KL) entre les distributions de sortie des différents modèles (plus la divergence est faible, plus les modèles sont en accord). Formule du score : Score = 0.5 × Précision Soft Voting + 0.3 × Confiance Soft Voting + 0.2 × (1 - Divergence KL)\n\nLes performances obtenues avec le stacking n'ont pas surpassé celles du soft voting.En raison de la complexité de mise en œuvre et de l'absence de gain notable, nous avons décidé d'abandonner cette méthode au profit de l'agrégation simple.\n\n- Limitations liées au périmètre des données : la première limitation concerne le périmètre des plantes et végétaux présents dans la base. Dès lors, l'application ne pourra pas être utilisée pour reconnaître « toutes » les espèces de plantes. La généralisation de l'application ne pourrait venir que de l'élargissement du périmètre des plantes concernées. - Limitations liées à l'hétérogénéité des données : la seconde limitation concerne l'hétérogénéité des données étudiées, qui a mené à l'exclusion des images de jeunes pousses trop éloignées de la majorité de la base d'images que nous avons pu constituer, et insuffisamment nombreuses pour les intégrer dans le jeu d'entraînement. De même, cela limitera la portée de l'application.\n\nCes difficultés techniques rencontrées avec SHAP et Grad-CAM sont aussi intrinsèquement liées à la complexité et à la profondeur de notre modèle EfficientNetV2M, qui comporte des couches spécifiques (DepthwiseConv2d, BiasAdd) rendant son interprétation plus difficile avec certains outils standards.", "answer_error": ""}
{"question": "Quels jeux de données ont été retenus ou exclus et pourquoi ?", "answer": "", "context": "- Limitations liées au périmètre des données : la première limitation concerne le périmètre des plantes et végétaux présents dans la base. Dès lors, l'application ne pourra pas être utilisée pour reconnaître « toutes » les espèces de plantes. La généralisation de l'application ne pourrait venir que de l'élargissement du périmètre des plantes concernées. - Limitations liées à l'hétérogénéité des données : la seconde limitation concerne l'hétérogénéité des données étudiées, qui a mené à l'exclusion des images de jeunes pousses trop éloignées de la majorité de la base d'images que nous avons pu constituer, et insuffisamment nombreuses pour les intégrer dans le jeu d'entraînement. De même, cela limitera la portée de l'application.\n\nLes datasets générés (filtré, relabellisé, augmenté) ont été utilisés pour affiner les modèles préalablement entraînés EfficientNetV2M, ResNet50V2 et Swintransformer. Le modèle Convnext étant relativement plus long à entraîner, nous avons décidé de l'exclure de cette étude d'amélioration.\n\nLes performances obtenues avec le stacking n'ont pas surpassé celles du soft voting.En raison de la complexité de mise en œuvre et de l'absence de gain notable, nous avons décidé d'abandonner cette méthode au profit de l'agrégation simple.\n\nEla CIMEN, Algorithmie = 2/5. Ela CIMEN, Dév. Appli mobile = 0/5. Ela CIMEN, Statistiques = 3/5. Ela CIMEN, Gestion projet = 4/5. Ela CIMEN, Botanique = 3/5. Roland KONAN, Algorithmie = 4/5. Roland KONAN, Dév. Appli mobile = 2/5. Roland KONAN, Statistiques = 3/5. Roland KONAN, Gestion projet = 4/5. Roland KONAN, Botanique = 2/5. Yacine MADI SAID, Algorithmie = 3/5. Yacine MADI SAID, Dév. Appli mobile = 5/5. Yacine MADI SAID, Statistiques = 2/5. Yacine MADI SAID, Gestion projet = 4/5. Yacine MADI SAID, Botanique = 1/5. Nicolas RINCÉ, Algorithmie = 2/5. Nicolas RINCÉ, Dév. Appli mobile = 0/5. Nicolas RINCÉ, Statistiques = 4/5. Nicolas RINCÉ, Gestion\n\n- Exploiter les datasets nettoyés et enrichis. - Adapter les modèles à une distribution de données plus fiable. - Mesurer directement l'impact des stratégies sur les performances, notamment sur les classes Cassava.\n\nDans certaines images, des zones d'attention sont visibles en dehors des feuilles, notamment sur l'arrière-plan ou sur d'autres objets. Cela suggère que le modèle peut être distrait par des éléments non pertinents.\n\n- Organisation du code pour améliorer le temps d'apprentissage - Augmentation de la data sur le modèle - Ajouts des poids sur les classes pour lutter contre le déséquilibre - Gel/Dégel du modèle de base - Shuffle passé à False dans le dataset de test Les métriques précision, recall et F1-score gagnent environ 1pt pour atteindre 95% et 98% pour respectivement EfficientNetB0 et VGG12. Ce dernier est donc plus performant, ce qui élimine le premier modèle. Bien que dans l'apprentissage de ces deux modèles, les classes 'Cassava' aient été exclues, on note des classes de tomates moins bien prédites telles que celles atteintes des maladies 'Mosaic Virus' et 'Spider Mites' (Précision : 73%, Recall : 98%, F1-Score : 84%). On note d'ailleurs qu'à l'oeil nu la résolution de certaines images ne permet pas de détecter ces maladies. Mosaic Virus\n\nUne fois les images traitées, le DataFrame Low_quality_processed contenant les informations des images traitées est enregistré dans un fichier CSV. Ce fichier peut être utilisé pour des analyses ultérieures ou pour vérifier les modifications apportées à chaque image.", "answer_error": ""}
{"question": "Quels critères de qualité d'image ont été utilisés (flou, contraste, luminosité) ?", "answer": "", "context": "Plusieurs traitements sont appliqués aux images dans le but d'améliorer leur qualité. Les traitements incluent des ajustements sur la luminosité, le contraste, la netteté (flou et netteté), ainsi que des redimensionnements. Les étapes sont détaillées ci-dessous :\n\n- Flou > 500 - Contraste > 2 - Luminosité < 200 - Nombre maximum d'image =12500 - nombre minimum d'image =1250 A partir du dataframe initial, on parcourt l'ensemble des images grâce au FilePath pour appliquer nos critères de qualité et créer un dataframe ( df_high_quality ) réunissant uniquement les images qui passent le test. Ceux ne passant pas les critères de qualité vont être intégré à un dataframe différent ( df_low_quality ) pour être traité.\n\nLes résultats suivants comparent les performances des modèles avant et après application des trois stratégies (filtrage, relabellisation, augmentation) sur l'ensemble des classes (global) et spécifiquement sur les classes Cassava.\n\nNous n'avons pas eu recours à des experts externes à proprement parler (en-dehors de Damien évidemment !) mais avons réalisé de nombreuses recherches afin de nous guider, notamment sur les sujets suivants : - Taille des images standard utilisée dans les algorithmes de reconnaissance d'image ; - Détermination du niveau de flou d'une image et seuil d'acceptabilité pour la reconnaissance d'images (150 de Laplacienne) ; - Retraitement des niveaux de flous ; - Déséquilibre acceptable entre classes sur le fichier d'entraînement (1 à 10) ; Nombre minimal d'images à soumettre à l'algorithme par classe (1.000).\n\n- Variables discriminantes : Identification du niveau de bleu comme prédicteur principal du caractère sain/malade (statistique t=128) - Seuils de qualité : Définition de critères quantitatifs pour la sélection d'images agricoles (flou >500, contraste >2) - Analyse des échecs : Documentation des limites des approches CNN sur certaines pathologies visuellement similaires\n\nL'application sera développée sur Streamlit conformément au schéma de l'application présentée en Page 6 en veillant à des règles d'ergonomie de « base » (éviter les images de fond, contraste des couleurs pour faciliter la lecture, usage des couleurs limité à une gamme réduite, boutons de navigation explicites, etc.) Jalons du projet\n\nAvant retraitements Après retraitements Les boîtes à moustaches montrent que le retraitement a amélioré la qualité des images en réduisant les valeurs aberrantes. Pour le contraste, la médiane est restée stable, mais les valeurs très faibles de contraste ont été corrigées, et la gamme des valeurs s'est élargie. De même, pour la luminosité, la médiane est inchangée, mais les images trop lumineuses ont été ajustées, créant une distribution plus homogène. En résumé, les traitements ont réduit les extrêmes tout en maintenant une stabilité de la médiane.\n\n- Couches convolutives et pooling : L'architecture commence par deux couches convolutives suivies de couches de max-pooling pour extraire et réduire les caractéristiques spatiales des images. Les tailles des filtres et des strides varient pour capturer différents niveaux de détails. - Couches convolutives supplémentaires : Trois couches convolutives supplémentaires sont ajoutées, suivies d'une couche de max-pooling, pour approfondir l'extraction des caractéristiques et capturer des motifs plus complexes dans les images. - Couches entièrement connectées et régularisation : Après l'aplatissement des caractéristiques, deux couches entièrement connectées avec des unités de dropout sont utilisées pour la classification. La sortie finale est une couche dense avec une activation softmax pour la classification multi-classes. La précision est de 80,40 % sur les données d'entraînement et de 85,40 % sur les données de test.", "answer_error": ""}
{"question": "Quel est le bilan global et les pistes d'amélioration proposées ?", "answer": "", "context": "Les résultats suivants comparent les performances des modèles avant et après application des trois stratégies (filtrage, relabellisation, augmentation) sur l'ensemble des classes (global) et spécifiquement sur les classes Cassava.\n\n- Diagnostic précoce : Détection automatisée des maladies pour intervention rapide - Réduction des intrants : Ciblage précis des traitements phytosanitaires - Optimisation des rendements : Prévention des pertes de récolte estimées à 20-40% selon la FAO\n\nLes modèles ResNet50V2 et Swin Transformer présentent la plus forte divergence (0.081), confirmant leur grande diversité de prédiction. EfficientNetV2M et Swin présentent également une divergence modérée (0.033), suggérant une complémentarité exploitable. À l'inverse, EfficientNetV2M et ConvNeXt sont plus proches (0.038), ce qui indique une certaine redondance dans leurs décisions. Ces résultats renforcent l'hypothèse selon laquelle un ensemblage de modèles hétérogènes (ResNet + Swin + EfficientNet) peut améliorer la robustesse des prédictions globales grâce à une diversité suffisante dans les sorties.\n\nLa perte (loss) semble globalement correcte car elle diminue au fil des époques, indiquant une amélioration du modèle, bien que des fluctuations soient observées sur les données d'entraînement. Précision Perte L'analyse des métriques de classification indique que le modèle ne performe pas bien, avec des scores F1 très faibles pour toutes les classes, une précision globale de seulement 0,03, et des valeurs moyennes macro et pondérées également très basses, ce qui suggère une mauvaise capacité de généralisation et de classification des différentes catégories, probablement due à un déséquilibre des classes dans les données . Matrice de confusion\n\nEfficientNetV2M, Stratégie = Base Jine. EfficientNetV2M, Accuracy Globale = 0.9858. EfficientNetV2M, Fl Score Global = 0.9845. EfficientNetV2M, Gain Accuracy = . EfficientNetV2M, Gain Fl Score = . , Stratégie = Augmentation. , Accuracy Globale = 0.9748. , Fl Score Global = 0.9773. , Gain Accuracy = 1.109. , Gain Fl Score = . , Stratégie = Filtrage. , Accuracy Globale = 0.9695. , Fl Score Global = 0.9773. , Gain Accuracy = 1.63%. , Gain Fl Score = -0.72%. , Stratégie = Relabeling. , Accuracy Globale = 0.9625. , Fl Score Global = 0.9731. , Gain Accuracy = -2.33%. , Gain Fl Score = 1.149. ResNetSOV2, Stratégie = Base Jine. ResNetSOV2, Accuracy Globale = 0.9578. ResNetSOV2,\n\nL'application sera développée sur Streamlit conformément au schéma de l'application présentée en Page 6 en veillant à des règles d'ergonomie de « base » (éviter les images de fond, contraste des couleurs pour faciliter la lecture, usage des couleurs limité à une gamme réduite, boutons de navigation explicites, etc.) Jalons du projet\n\nNous n'avons pas eu recours à des experts externes à proprement parler (en-dehors de Damien évidemment !) mais avons réalisé de nombreuses recherches afin de nous guider, notamment sur les sujets suivants : - Taille des images standard utilisée dans les algorithmes de reconnaissance d'image ; - Détermination du niveau de flou d'une image et seuil d'acceptabilité pour la reconnaissance d'images (150 de Laplacienne) ; - Retraitement des niveaux de flous ; - Déséquilibre acceptable entre classes sur le fichier d'entraînement (1 à 10) ; Nombre minimal d'images à soumettre à l'algorithme par classe (1.000).\n\nLe graphique ci-contre montre l'importance des variables dans la contribution à la classification et confirme en contributeur principal le niveau de bleu. Le taux de bonne prédiction de la classe malade est de 93,5% mais le modèle ne fonctionne pas correctement pour le classement des images saines (taux de prédiction de 74%) Un premier modèle Random Forest ent pour le classement des images saines (taux de prédiction de", "answer_error": ""}
{"question": "Quel modèle offre le meilleur compromis précision / vitesse d'inférence ?", "answer": "", "context": "Les modèles ResNet50V2 et Swin Transformer ont une corrélation d'erreurs faible (0.31), ce qui indique une forte complémentarité. EfficientNetV2M et ConvNeXt présentent également une diversité modérée dans leurs erreurs vis-à-vis de ResNet50V2. Le score le plus élevé entre deux modèles différents est celui entre Swin et ConvNeXt (0.51), ce qui reste suffisamment bas pour justifier un ensemblage. Ces résultats confirment l'intérêt d'un ensemblage basé sur ces modèles, car la diversité des erreurs est un facteur favorable à l'amélioration globale des performances. Pour compléter l'analyse, nous avons mesuré la divergence de Jensen-Shannon (JSD) entre les distributions de probabilités prédites par les différents modèles.\n\n- Quantification avancée : Techniques de compression préservant mieux les performances - Edge computing : Optimisation pour calcul embarqué sur capteurs IoT agricoles - Traitement en temps réel : Pipeline optimisé pour diagnostic instantané\n\nLe modèle ne focalise pas systématiquement sur les mêmes zones d'une image à l'autre, ce qui reflète à la fois une adaptation aux différences d'images et un possible manque de stabilité dans la reconnaissance.\n\nLa perte (loss) semble globalement correcte car elle diminue au fil des époques, indiquant une amélioration du modèle, bien que des fluctuations soient observées sur les données d'entraînement. Précision Perte L'analyse des métriques de classification indique que le modèle ne performe pas bien, avec des scores F1 très faibles pour toutes les classes, une précision globale de seulement 0,03, et des valeurs moyennes macro et pondérées également très basses, ce qui suggère une mauvaise capacité de généralisation et de classification des différentes catégories, probablement due à un déséquilibre des classes dans les données . Matrice de confusion\n\n- Organisation du code pour améliorer le temps d'apprentissage - Augmentation de la data sur le modèle - Ajouts des poids sur les classes pour lutter contre le déséquilibre - Gel/Dégel du modèle de base - Shuffle passé à False dans le dataset de test Les métriques précision, recall et F1-score gagnent environ 1pt pour atteindre 95% et 98% pour respectivement EfficientNetB0 et VGG12. Ce dernier est donc plus performant, ce qui élimine le premier modèle. Bien que dans l'apprentissage de ces deux modèles, les classes 'Cassava' aient été exclues, on note des classes de tomates moins bien prédites telles que celles atteintes des maladies 'Mosaic Virus' et 'Spider Mites' (Précision : 73%, Recall : 98%, F1-Score : 84%). On note d'ailleurs qu'à l'oeil nu la résolution de certaines images ne permet pas de détecter ces maladies. Mosaic Virus\n\nNous avons exploré deux approches principales pour combiner les modèles : Soft Voting (vote pondéré par les probabilités) : chaque modèle contribue à la prédiction finale via sa distribution de probabilité. Nous avons initialement utilisé une pondération équivalente pour chaque modèle. Une étude systématique des coefficients de pondération n'a pas mis en évidence de combinaison significativement meilleure que l'équipondération. Cette méthode s'est avérée simple, stable, et a permis d'obtenir des résultats compétitifs. Stacking : cette approche consistait à utiliser un méta-modèle entraîné à partir des sorties des modèles de base.\n\nCes difficultés techniques rencontrées avec SHAP et Grad-CAM sont aussi intrinsèquement liées à la complexité et à la profondeur de notre modèle EfficientNetV2M, qui comporte des couches spécifiques (DepthwiseConv2d, BiasAdd) rendant son interprétation plus difficile avec certains outils standards.\n\nSelon les modèles et afin de nous assurer, malgré le faible déséquilibre de classes, que les classes les moins représentées soient correctement traitées, nous avons observé dans certains cas quelle était la k-ème Accuracy la plus faible parmi les classes proposées au modèle.", "answer_error": ""}
{"question": "Pourquoi le stacking n'a-t-il pas surpassé le soft voting ?", "answer": "", "context": "Les performances obtenues avec le stacking n'ont pas surpassé celles du soft voting.En raison de la complexité de mise en œuvre et de l'absence de gain notable, nous avons décidé d'abandonner cette méthode au profit de l'agrégation simple.\n\nNous avons exploré deux approches principales pour combiner les modèles : Soft Voting (vote pondéré par les probabilités) : chaque modèle contribue à la prédiction finale via sa distribution de probabilité. Nous avons initialement utilisé une pondération équivalente pour chaque modèle. Une étude systématique des coefficients de pondération n'a pas mis en évidence de combinaison significativement meilleure que l'équipondération. Cette méthode s'est avérée simple, stable, et a permis d'obtenir des résultats compétitifs. Stacking : cette approche consistait à utiliser un méta-modèle entraîné à partir des sorties des modèles de base.\n\nEfficientNetV2M reste central dans tous les meilleurs ensembles : c'est le modèle le plus constant et performant, en global comme sur Cassava. ResNet50V2, bien que moins performant seul, apporte une complémentarité utile en ensemble, notamment sur les classes Cassava. Le stacking n'a pas permis d'amélioration notable par rapport au soft voting, tout en nécessitant une mise en œuvre plus complexe. Il a donc été écarté dans les configurations finales. En tenant compte des contraintes pratiques de mise en œuvre de notre application (réduction des temps d'entraînement, coûts d'hébergement, vitesse d'inférence) nous avons décidé de sélectionner l'ensemble EfficientNetV2M + ResNet50V2.\n\n- Les images du premier décile de score ont été re-labellisées automatiquement. - Le nouveau label correspond à la classe prédite par le soft voting, considérée comme plus fiable que le label d'origine. - Les autres images ont conservé leur annotation initiale. Objectifs - Corriger les labels potentiellement erronés. - Maintenir la taille du dataset en remplaçant les étiquettes douteuses. - Exploiter la confiance collective des modèles pour guider la correction.\n\nEla CIMEN, Algorithmie = 2/5. Ela CIMEN, Dév. Appli mobile = 0/5. Ela CIMEN, Statistiques = 3/5. Ela CIMEN, Gestion projet = 4/5. Ela CIMEN, Botanique = 3/5. Roland KONAN, Algorithmie = 4/5. Roland KONAN, Dév. Appli mobile = 2/5. Roland KONAN, Statistiques = 3/5. Roland KONAN, Gestion projet = 4/5. Roland KONAN, Botanique = 2/5. Yacine MADI SAID, Algorithmie = 3/5. Yacine MADI SAID, Dév. Appli mobile = 5/5. Yacine MADI SAID, Statistiques = 2/5. Yacine MADI SAID, Gestion projet = 4/5. Yacine MADI SAID, Botanique = 1/5. Nicolas RINCÉ, Algorithmie = 2/5. Nicolas RINCÉ, Dév. Appli mobile = 0/5. Nicolas RINCÉ, Statistiques = 4/5. Nicolas RINCÉ, Gestion\n\nL'application sera développée sur Streamlit conformément au schéma de l'application présentée en Page 6 en veillant à des règles d'ergonomie de « base » (éviter les images de fond, contraste des couleurs pour faciliter la lecture, usage des couleurs limité à une gamme réduite, boutons de navigation explicites, etc.) Jalons du projet\n\n- Du point de vue scientifique, l'utilisation de cette application pourrait offrir un support durable au recensement des espèces, renforcer l'échange de connaissances entre chercheurs, en particulier sur le maintien de la bio-diversité, sensibiliser écoliers, étudiants et grand public à la connaissance des espèces et leur conservation, être élargi à la recherche des causes des maladies à l'aide de données additionnelles (géographie, climat, …)\n\n- Incompatibilités techniques : Problèmes de compatibilité entre TensorFlow 2.19 et certains outils d'interprétation (SHAP DeepExplainer, GradCam) - Migration entre frameworks : Passage de FastAI vers TensorFlow pour faciliter l'intégration Firebase, nécessitant la ré-implémentation de certains modèles", "answer_error": ""}
{"question": "Comment les cartes de saillance ont-elles aidé l'interprétation des modèles ?", "answer": "", "context": "Pour comprendre les décisions de notre modèle VGG16, nous allons utiliser 3 outils d'interprétation : - -Saliency, qui permet de mettre en évidence les pixels les plus importants d'une image pour une prédiction donnée. - -SHAP (SHapley Additive exPlanations), pour visualiser les caractéristiques qui permettent la prédictions d'une classe sur une image. - -LIME (Local Interpretable Model-agnostic Explanations), qui donne les caractéristiques d'une image qui vont le plus influencer la décision du model. D'après nos outils d'interprétation, on remarque que les saillances désignent bien les parties des images importantes pour chacune des classes. Elle reconnaît les contours des plantes et les différences de couleurs issues des maladies.\n\n- Exploiter les datasets nettoyés et enrichis. - Adapter les modèles à une distribution de données plus fiable. - Mesurer directement l'impact des stratégies sur les performances, notamment sur les classes Cassava.\n\nLe modèle ne focalise pas systématiquement sur les mêmes zones d'une image à l'autre, ce qui reflète à la fois une adaptation aux différences d'images et un possible manque de stabilité dans la reconnaissance.\n\nAvant retraitements Après retraitements Les boîtes à moustaches montrent que le retraitement a amélioré la qualité des images en réduisant les valeurs aberrantes. Pour le contraste, la médiane est restée stable, mais les valeurs très faibles de contraste ont été corrigées, et la gamme des valeurs s'est élargie. De même, pour la luminosité, la médiane est inchangée, mais les images trop lumineuses ont été ajustées, créant une distribution plus homogène. En résumé, les traitements ont réduit les extrêmes tout en maintenant une stabilité de la médiane.\n\nSelon les modèles et afin de nous assurer, malgré le faible déséquilibre de classes, que les classes les moins représentées soient correctement traitées, nous avons observé dans certains cas quelle était la k-ème Accuracy la plus faible parmi les classes proposées au modèle.\n\nLe principal verrou scientifique de ce projet s'est révélé être la qualité et la labellisation hétérogène des données d'images, particulièrement pour la classe Cassave, qui a systématiquement montré des performances de classification inférieures malgré les efforts de fine-tuning. Cette difficulté reflète un défi majeur en reconnaissance d'images agricoles : distinguer les symptômes subtils de maladies sur des images prises dans des conditions naturelles variables, avec des arrière-plans complexes et des variations d'éclairage . Difficultés détaillées par catégorie\n\nNous avons exploré deux approches principales pour combiner les modèles : Soft Voting (vote pondéré par les probabilités) : chaque modèle contribue à la prédiction finale via sa distribution de probabilité. Nous avons initialement utilisé une pondération équivalente pour chaque modèle. Une étude systématique des coefficients de pondération n'a pas mis en évidence de combinaison significativement meilleure que l'équipondération. Cette méthode s'est avérée simple, stable, et a permis d'obtenir des résultats compétitifs. Stacking : cette approche consistait à utiliser un méta-modèle entraîné à partir des sorties des modèles de base.\n\nCassava_green mottle, 4 = 279. Cassava nosaic disease, 1 = 91. Cassava nosaic disease, 2 = 0 89. Cassava nosaic disease, 3 = 0 .90. Cassava nosaic disease, 4 = 1540 Nous avons donc tâché de comprendre pourquoi le modèle classifie mal en regardant plus en détail les images de Cassave. Nous avons choisi d'utiliser la méthode de saillance native à TensorFlow, car elle est pleinement compatible avec notre environnement et ne présente pas de problèmes d'incompatibilité, ce qui facilite son intégration.", "answer_error": ""}
{"question": "Quelles limites du dataset ont impacté la généralisation ?", "answer": "", "context": "- Limitations liées au périmètre des données : la première limitation concerne le périmètre des plantes et végétaux présents dans la base. Dès lors, l'application ne pourra pas être utilisée pour reconnaître « toutes » les espèces de plantes. La généralisation de l'application ne pourrait venir que de l'élargissement du périmètre des plantes concernées. - Limitations liées à l'hétérogénéité des données : la seconde limitation concerne l'hétérogénéité des données étudiées, qui a mené à l'exclusion des images de jeunes pousses trop éloignées de la majorité de la base d'images que nous avons pu constituer, et insuffisamment nombreuses pour les intégrer dans le jeu d'entraînement. De même, cela limitera la portée de l'application.\n\nLes différentes stratégies mises en place visaient à améliorer la classification des images du dataset Cassava, en ciblant notamment les classes déséquilibrées ou mal étiquetées. Cependant, les résultats montrent que ces méthodes ont globalement entraîné une dégradation des performances générales, quel que soit le modèle utilisé. La perte en accuracy et en F1-score est visible, particulièrement sur EfficientNetV2M, qui perd jusqu'à 2.3 % d'accuracy globale. Bien que certaines stratégies, comme l'augmentation massive, aient permis des gains localisés (notamment sur le F1-score des classes Cassava), ces améliorations ne compensent pas les pertes sur l'ensemble des classes. Nous décidons dans la suite de garder nos modèles baseline.\n\n- Extension du dataset : I ntégration de bases de données supplémentaires pour couvrir plus d'espèces (objectif : passer de 19 à 100+ espèces) - Augmentation ciblée : Techniques d'augmentation spécialisées par type de maladie (GANs, style transfer) - Données multi-modales : Intégration d'informations contextuelles (géolocalisation, saison, …)\n\nLes datasets générés (filtré, relabellisé, augmenté) ont été utilisés pour affiner les modèles préalablement entraînés EfficientNetV2M, ResNet50V2 et Swintransformer. Le modèle Convnext étant relativement plus long à entraîner, nous avons décidé de l'exclure de cette étude d'amélioration.\n\nWidth Width Avant retraitement, les images avaient des dimensions très variées, ce qui compliquait leur analyse et leur traitement uniforme. Cette hétérogénéité des tailles pouvait entraîner des biais lors de l'application de modèles d'apprentissage automatique ou d'autres analyses d'images, car les réseaux de neurones et les algorithmes de traitement d'images nécessitent des entrées de taille uniforme. Après retraitement, toutes les images ont été redimensionnées à une taille standard de 256x256 pixels, un format couramment utilisé dans la datascience. Cette normalisation permet non seulement de faciliter l'analyse et l'entraînement des modèles, mais elle garantit également que toutes les images sont traitées de manière cohérente, améliorant ainsi la performance des modèles d'analyse et de classification.\n\n- 1) La perte de validation suit une tendance à la baisse, passant de 0.3093 à 0.1281 . Rejoignant la perte d'entraînement, ce qui suppose une bonne généralisation du modèle - 2) La précision croît jusqu'à atteindre 95% sur 5 epoch seulement, avec un taux d'erreur à 4% , ce qui implique une très bonne performance du modèle. 3Torch\n\n- ∞ Temps de préprocessing sous-estimé : Le nettoyage et l'homogénéisation des multiples datasets (fusion de bases redondantes, correction des déséquilibres entre classes) ont nécessité significativement plus de temps que prévu - ∞ Complexité du fine-tuning : Les itérations successives d'optimisation des modèles, particulièrement pour traiter les classes minoritaires comme la Cassave, ont multiplié les cycles de développement par 3 - ∞ Phase d'interprétation des modèles : L'utilisation d'outils comme GradCam et SHAP s'est révélée plus complexe techniquement qu'anticipé, avec des incompatibilités entre certaines architectures (EfficientNetV2M) et les outils d'explicabilité\n\n- Limitations liées au déséquilibre des classes : la troisième limitation concerne le déséquilibre entre plantes avec une surreprésentation de certaines espèces dont en particulier les tomates. - Limitations liées à la qualité des données : la quatrième limitation touche la qualité des données parmi les images retenues, et notamment leur degré de flou qui est primordial dans la reconnaissance des images - Limitations liées à la taille du dataset : la cinquième limitation concerne la taille de l'ensemble des images ainsi constituées qui doit être limitée afin de tenir compte des temps de traitement de l'algorithme.", "answer_error": ""}
{"question": "Quelles sont les conclusions sur l'inclusion de la classe Cassava dans le modèle final ?", "answer": "", "context": "Les résultats suivants comparent les performances des modèles avant et après application des trois stratégies (filtrage, relabellisation, augmentation) sur l'ensemble des classes (global) et spécifiquement sur les classes Cassava.\n\nCompte tenu du faible % de prédiction, la Cassave a fait l'objet d'un focus particulier. On constate que le finetuning réalisé a eu un effet spectaculaire sur la reconnaissance de ces classes (hausse du F1-score de 15pts environ chez EfficientNetV2M et ViT16) notamment sur la Cassave saine. S'agissant des autres classes, on relève les principaux enseignements suivants:\n\n- Choix architecturaux : Nécessité de réviser l'approche initiale face aux performances insuffisantes sur certaines classes, menant à l'exploration de modèles alternatifs (SwinTransformer) ou l'utilisation de certaines images pré-retraitement (Cassave) - Métriques d'évaluation : Remise en question de l'accuracy comme métrique principale face au déséquilibre des classes et utilisation du F1-score - Stratégie de données : Questionnement sur l'inclusion/exclusion de la classe Cassave dans le modèle final - Puissance computationnelle : Temps d'entraînement prohibitifs pour certains modèles (EfficientNetV2M, VGG16) nécessitant l'optimisation du code et la réduction de la taille des images - Mémoire : Contraintes mémoire lors de l'entraînement de modèles profonds avec des images haute résolution - Déploiement : Défis de compression des modèles pour un usage mobile tout en préservant les performances\n\n- Apporter de la diversité visuelle aux classes sous-représentées. - Réduire le biais induit par la classe majoritaire. - Améliorer la généralisation du modèle sur les classes rares. 2. Filtrage basé sur un score multi-critères Une seconde approche a consisté à filtrer les images Cassava jugées peu fiables à l'aide d'un score multi-critères.\n\nUne attention particulière sera portée sur les classes de cassave pour lesquelles le modèle obtient les moins bons résultats. Cela permettra d'identifier les causes des erreurs et d'améliorer la robustesse du modèle.\n\nRésultats Cassava.models = EffNetV2M + ConvNeXt. , accuracy. = 0.9049. , fl score. = 0.8487. , nb_models cassava_only. = True. , Résultats Cassava.rank = . , Résultats Cassava.models = EffNetV2M Swin. , accuracy. = 0.9099. , fl score. = 0.8486. , nb_models cassava_only. = True. , Résultats Cassava.rank = . , Résultats Cassava.models = EffNetV2M. , accuracy. = 0.9068. , fl score. = 0.8432. , nb_models cassava_only. = True. , Résultats Cassava.rank = . , Résultats Cassava.models = ConvNeXt ResNetSovz Swin. , accuracy. = 0.8805. , fl score. = 0.8092. , nb_models cassava_only. = True. , Résultats Cassava.rank = . , Résultats Cassava.models = ConvNeXt +\n\nLes différentes stratégies mises en place visaient à améliorer la classification des images du dataset Cassava, en ciblant notamment les classes déséquilibrées ou mal étiquetées. Cependant, les résultats montrent que ces méthodes ont globalement entraîné une dégradation des performances générales, quel que soit le modèle utilisé. La perte en accuracy et en F1-score est visible, particulièrement sur EfficientNetV2M, qui perd jusqu'à 2.3 % d'accuracy globale. Bien que certaines stratégies, comme l'augmentation massive, aient permis des gains localisés (notamment sur le F1-score des classes Cassava), ces améliorations ne compensent pas les pertes sur l'ensemble des classes. Nous décidons dans la suite de garder nos modèles baseline.\n\nOn constate une baisse de l'Accuracy de 75,02% à 70,02% au terme du fine-tuning. La classification s'améliore sur l'ensemble des classes sauf sur la Tomate, la Cassave et la Canne à sucre. La faible Accuracy et la longueur de l'entraînement nous font conclure à l'abandon du modèle.", "answer_error": ""}
{"question": "Quelle a été la performance sur les sous-classes Cassava ?", "answer": "", "context": "Au vu des performances modestes obtenues par nos modèles sur le sous-ensemble Cassava, nous avons décidé de mettre en œuvre plusieurs stratégies d'amélioration. Ces approches visent à renforcer la qualité des données et à améliorer la capacité des modèles à discriminer les classes Cassava, souvent déséquilibrées et visuellement similaires.\n\nCompte tenu du faible % de prédiction, la Cassave a fait l'objet d'un focus particulier. On constate que le finetuning réalisé a eu un effet spectaculaire sur la reconnaissance de ces classes (hausse du F1-score de 15pts environ chez EfficientNetV2M et ViT16) notamment sur la Cassave saine. S'agissant des autres classes, on relève les principaux enseignements suivants:\n\nResNet50V2, Accuracy = 0.96. ResNet50V2, Macro F1- score = 0.96. ResNet50V2, Weighted F1 = 0.96. ResNet50V2, Observations clés = Bons résultats globaux, mais performance moyenne sur les classes Cassava. EfficientNetV2M, Accuracy = 0.99. EfficientNetV2M, Macro F1- score = 0.98. EfficientNetV2M, Weighted F1 = 0.99. EfficientNetV2M, Observations clés = Meilleure performance globale, bonne gestion des classes déséquilibrées comme Cassava. ViT16, Accuracy = 0.98. ViT16, Macro F1- score = 0.97. ViT16, Weighted F1 = 0.98. ViT16, Observations clés = Meilleure performance globale, bonne gestion des classes déséquilibrées comme Cassava\n\nLes résultats suivants comparent les performances des modèles avant et après application des trois stratégies (filtrage, relabellisation, augmentation) sur l'ensemble des classes (global) et spécifiquement sur les classes Cassava.\n\n- Performances globales : légère dégradation sur l'ensemble des modèles. - Cassava : amélioration significative du F1-score pour SwinTransformer (+8.4%) et ResNet (+4.3%), malgré une légère baisse d'accuracy.\n\nLe principal verrou scientifique de ce projet s'est révélé être la qualité et la labellisation hétérogène des données d'images, particulièrement pour la classe Cassave, qui a systématiquement montré des performances de classification inférieures malgré les efforts de fine-tuning. Cette difficulté reflète un défi majeur en reconnaissance d'images agricoles : distinguer les symptômes subtils de maladies sur des images prises dans des conditions naturelles variables, avec des arrière-plans complexes et des variations d'éclairage . Difficultés détaillées par catégorie\n\n- Choix architecturaux : Nécessité de réviser l'approche initiale face aux performances insuffisantes sur certaines classes, menant à l'exploration de modèles alternatifs (SwinTransformer) ou l'utilisation de certaines images pré-retraitement (Cassave) - Métriques d'évaluation : Remise en question de l'accuracy comme métrique principale face au déséquilibre des classes et utilisation du F1-score - Stratégie de données : Questionnement sur l'inclusion/exclusion de la classe Cassave dans le modèle final - Puissance computationnelle : Temps d'entraînement prohibitifs pour certains modèles (EfficientNetV2M, VGG16) nécessitant l'optimisation du code et la réduction de la taille des images - Mémoire : Contraintes mémoire lors de l'entraînement de modèles profonds avec des images haute résolution - Déploiement : Défis de compression des modèles pour un usage mobile tout en préservant les performances\n\nLa troisième étape s'est avérée la plus significative en matière de gain de performance, avec un traitement spécifique apportée à la classe 'Cassava' en augmentant manuellement les images des classes minoritaires et en appliquant une augmentation algorithmique ciblée. Les résultats du fine-tuning par modèle sont résumés dans le tableau ci-dessous, les deux modèles EfficientNetV2M et ViT16 se distinguant sur les classes déséquilibrées comme la Cassave.", "answer_error": ""}
{"question": "Quelles méthodes ont amélioré (ou dégradé) le F1-score sur Cassava ?", "answer": "", "context": "Les différentes stratégies mises en place visaient à améliorer la classification des images du dataset Cassava, en ciblant notamment les classes déséquilibrées ou mal étiquetées. Cependant, les résultats montrent que ces méthodes ont globalement entraîné une dégradation des performances générales, quel que soit le modèle utilisé. La perte en accuracy et en F1-score est visible, particulièrement sur EfficientNetV2M, qui perd jusqu'à 2.3 % d'accuracy globale. Bien que certaines stratégies, comme l'augmentation massive, aient permis des gains localisés (notamment sur le F1-score des classes Cassava), ces améliorations ne compensent pas les pertes sur l'ensemble des classes. Nous décidons dans la suite de garder nos modèles baseline.\n\n- Performances globales : légère dégradation sur l'ensemble des modèles. - Cassava : amélioration significative du F1-score pour SwinTransformer (+8.4%) et ResNet (+4.3%), malgré une légère baisse d'accuracy.\n\n- Apporter de la diversité visuelle aux classes sous-représentées. - Réduire le biais induit par la classe majoritaire. - Améliorer la généralisation du modèle sur les classes rares. 2. Filtrage basé sur un score multi-critères Une seconde approche a consisté à filtrer les images Cassava jugées peu fiables à l'aide d'un score multi-critères.\n\nCompte tenu du faible % de prédiction, la Cassave a fait l'objet d'un focus particulier. On constate que le finetuning réalisé a eu un effet spectaculaire sur la reconnaissance de ces classes (hausse du F1-score de 15pts environ chez EfficientNetV2M et ViT16) notamment sur la Cassave saine. S'agissant des autres classes, on relève les principaux enseignements suivants:\n\nLa première phase a reposé sur : - des corrections d'erreur : mesures de précision sur des images test et non les images d'entraînement ; - des 'élagages' rapides de modèles jugés non pertinents ; - l'utilisation de techniques de fine-tuning : choix des 'optimizers', gel/dégel de couches des modèles pour l'apprentissage, technique d'augmentation de data, ajustement des learning rates et introduction de scheduler, modification des fonctions de 'loss', techniques de callback, application de poids aux classes pour corriger les déséquilibres ; - des ajustements techniques pour un apprentissage plus rapide : réorganisation du code ; - des choix réalisés pour le traitement de classes dont la prédiction est plus complexe : il s'agit principalement de la classe 'Cassava' avec la suppression pure et simple de ces classes ou bien focus sur la précision augmentée des modèles au fur et à mesure du fine-tuning.\n\nAu vu des performances modestes obtenues par nos modèles sur le sous-ensemble Cassava, nous avons décidé de mettre en œuvre plusieurs stratégies d'amélioration. Ces approches visent à renforcer la qualité des données et à améliorer la capacité des modèles à discriminer les classes Cassava, souvent déséquilibrées et visuellement similaires.\n\n- Organisation du code pour améliorer le temps d'apprentissage - Augmentation de la data sur le modèle - Ajouts des poids sur les classes pour lutter contre le déséquilibre - Gel/Dégel du modèle de base - Shuffle passé à False dans le dataset de test Les métriques précision, recall et F1-score gagnent environ 1pt pour atteindre 95% et 98% pour respectivement EfficientNetB0 et VGG12. Ce dernier est donc plus performant, ce qui élimine le premier modèle. Bien que dans l'apprentissage de ces deux modèles, les classes 'Cassava' aient été exclues, on note des classes de tomates moins bien prédites telles que celles atteintes des maladies 'Mosaic Virus' et 'Spider Mites' (Précision : 73%, Recall : 98%, F1-Score : 84%). On note d'ailleurs qu'à l'oeil nu la résolution de certaines images ne permet pas de détecter ces maladies. Mosaic Virus\n\n(Amélioration 1), Détails supplémentaires = 20 epochs max de finetuning Troisième Entraînement (Amélioration 2), 1 = AdamW. Troisième Entraînement (Amélioration 2), 2 = CosineDecayRestarts. Troisième Entraînement (Amélioration 2), 3 = SparseFocalLoss (avec poids des classes). Troisième Entraînement (Amélioration 2), 4 = earlystop, time_callback, printLR, model_checkpoint_ callback. Troisième Entraînement (Amélioration 2), 5 = 20 epochs max de finetuning Augmentation manuelle des images des classes minoritaires sur Cassava + augmentation ciblée sur ces classes", "answer_error": ""}
{"question": "Quels écarts de performance entre EfficientNetV2M, ResNet50V2 et SwinTransformer ?", "answer": "", "context": "Les modèles ResNet50V2 et Swin Transformer ont une corrélation d'erreurs faible (0.31), ce qui indique une forte complémentarité. EfficientNetV2M et ConvNeXt présentent également une diversité modérée dans leurs erreurs vis-à-vis de ResNet50V2. Le score le plus élevé entre deux modèles différents est celui entre Swin et ConvNeXt (0.51), ce qui reste suffisamment bas pour justifier un ensemblage. Ces résultats confirment l'intérêt d'un ensemblage basé sur ces modèles, car la diversité des erreurs est un facteur favorable à l'amélioration globale des performances. Pour compléter l'analyse, nous avons mesuré la divergence de Jensen-Shannon (JSD) entre les distributions de probabilités prédites par les différents modèles.\n\n- Performances globales : scores stables sur SwinTransformer et EfficientNetV2M, mais déclin marqué sur ResNet. - Cassava : amélioration du F1-score sur SwinTransformer (+5.8%), mais chute sévère sur ResNet (-15%).\n\n- Architecture : EfficientNet utilise une méthode d'optimisation appelée \"compound scaling\", qui ajuste simultanément la profondeur, la largeur et la résolution de l'image, offrant ainsi une meilleure efficacité énergétique et une précision supérieure. - Profundité : Moins profond que ResNet50V2, mais beaucoup plus efficace pour les mêmes performances. - Performance : Généralement plus rapide et plus efficace que ResNet50V2 pour la même précision. - Avantages : Très efficace en termes de taille du modèle et d'utilisation des ressources, il donne de bons résultats même avec des réseaux moins profonds. - Inconvénients : Moins éprouvé que ResNet50V2 dans certaines configurations.\n\nPrécision globale, Gagnant = EfficientNetV2M. Précision globale, Commentaire = Excellent suivi de Vit16. Robustesse sur classes rares, Gagnant = EfficientNetV2M. Robustesse sur classes rares, Commentaire = Meilleur équilibre F1. Performances Cassava, Gagnant = EfficientNetV2M. Performances Cassava, Commentaire = Résiste mieux au déséquilibre. Simplicité et rapidité entraînement, Gagnant = ResNet50V2. Simplicité et rapidité entraînement, Commentaire = Moins lourd, mais moins performant. Durée d'inférence, Gagnant = ResNet50V2. Durée d'inférence, Commentaire = 4 fois plus rapide que EfficientNetV2M et 6 fois plus rapide que ViT16. Taille disque du modèle, Gagnant = ViT16. Taille disque du modèle, Commentaire = Le plus léger suivi de ResNet50V2 , EfficientNetV2M est deux fois plus\n\nGlobalement, l'ensemble EfficientNetV2M + ResNet50V2 + SwinTransformer atteint les meilleures performances (accuracy = 0.9865, F1-score = 0.9856), surpassant tous les modèles individuels. Cela justifie pleinement l'intérêt du soft voting entre modèles complémentaires. Pour les classes Cassava uniquement, l'ensemble EfficientNetV2M + ResNet50V2 se démarque légèrement, avec un F1-score de 0.8674. Il est suivi de très près par l'ensemble intégrant SwinTransformer (0.8658), puis par EfficientNetV2M seul (0.8655). Ces résultats montrent que : Ajouter SwinTransformer à l'ensemble améliore les performances globales sans dégrader la spécialisation sur Cassava.\n\nLes datasets générés (filtré, relabellisé, augmenté) ont été utilisés pour affiner les modèles préalablement entraînés EfficientNetV2M, ResNet50V2 et Swintransformer. Le modèle Convnext étant relativement plus long à entraîner, nous avons décidé de l'exclure de cette étude d'amélioration.\n\n- ∞ Courbe d'apprentissage des architectures avancées : Maîtrise de modèles complexes comme SwinTransformer et EfficientNetV2M plus longue qu'anticipé - ∞ Interprétabilité des modèles : Compétences en explicabilité IA (SHAP, LIME, GradCam) acquises en cours de projet pour répondre aux exigences d'interprétation, parfois en avance de phase par rapport aux sprints du programme. - ∞ Optimisation pour le déploiement mobile : Apprentissage des techniques de compression et quantification de modèles pour l'usage sur smartphone\n\nLes modèles ResNet50V2 et Swin Transformer présentent la plus forte divergence (0.081), confirmant leur grande diversité de prédiction. EfficientNetV2M et Swin présentent également une divergence modérée (0.033), suggérant une complémentarité exploitable. À l'inverse, EfficientNetV2M et ConvNeXt sont plus proches (0.038), ce qui indique une certaine redondance dans leurs décisions. Ces résultats renforcent l'hypothèse selon laquelle un ensemblage de modèles hétérogènes (ResNet + Swin + EfficientNet) peut améliorer la robustesse des prédictions globales grâce à une diversité suffisante dans les sorties.", "answer_error": ""}
{"question": "Que révèlent les cartes de saillance/Grad-CAM sur les zones d'attention ?", "answer": "", "context": "Pour comprendre les décisions de notre modèle VGG16, nous allons utiliser 3 outils d'interprétation : - -Saliency, qui permet de mettre en évidence les pixels les plus importants d'une image pour une prédiction donnée. - -SHAP (SHapley Additive exPlanations), pour visualiser les caractéristiques qui permettent la prédictions d'une classe sur une image. - -LIME (Local Interpretable Model-agnostic Explanations), qui donne les caractéristiques d'une image qui vont le plus influencer la décision du model. D'après nos outils d'interprétation, on remarque que les saillances désignent bien les parties des images importantes pour chacune des classes. Elle reconnaît les contours des plantes et les différences de couleurs issues des maladies.\n\nDans certaines images, des zones d'attention sont visibles en dehors des feuilles, notamment sur l'arrière-plan ou sur d'autres objets. Cela suggère que le modèle peut être distrait par des éléments non pertinents.\n\n- Jardinage amateur : Identification et conseil pour jardiniers particuliers - Éducation environnementale : Sensibilisation à la biodiversité végétale - Surveillance phytosanitaire participative : Collecte de données citoyennes pour le monitoring des maladies émergentes\n\nSelon les modèles et afin de nous assurer, malgré le faible déséquilibre de classes, que les classes les moins représentées soient correctement traitées, nous avons observé dans certains cas quelle était la k-ème Accuracy la plus faible parmi les classes proposées au modèle.\n\n- Couches convolutives et pooling : L'architecture commence par deux couches convolutives suivies de couches de max-pooling pour extraire et réduire les caractéristiques spatiales des images. Les tailles des filtres et des strides varient pour capturer différents niveaux de détails. - Couches convolutives supplémentaires : Trois couches convolutives supplémentaires sont ajoutées, suivies d'une couche de max-pooling, pour approfondir l'extraction des caractéristiques et capturer des motifs plus complexes dans les images. - Couches entièrement connectées et régularisation : Après l'aplatissement des caractéristiques, deux couches entièrement connectées avec des unités de dropout sont utilisées pour la classification. La sortie finale est une couche dense avec une activation softmax pour la classification multi-classes. La précision est de 80,40 % sur les données d'entraînement et de 85,40 % sur les données de test.\n\nOn constate que les variables de luminosité et de niveau de rouge, vert et bleu sont très bien corrélées. En revanche, à l'exception du couple ('Blur','Filesize'), les autres variables sont peu corrélées entre elles. Heatmap of Correlation Matrix\n\nLe principal verrou scientifique de ce projet s'est révélé être la qualité et la labellisation hétérogène des données d'images, particulièrement pour la classe Cassave, qui a systématiquement montré des performances de classification inférieures malgré les efforts de fine-tuning. Cette difficulté reflète un défi majeur en reconnaissance d'images agricoles : distinguer les symptômes subtils de maladies sur des images prises dans des conditions naturelles variables, avec des arrière-plans complexes et des variations d'éclairage . Difficultés détaillées par catégorie\n\nResNet50V2, Accuracy = 0.96. ResNet50V2, Macro F1- score = 0.96. ResNet50V2, Weighted F1 = 0.96. ResNet50V2, Observations clés = Bons résultats globaux, mais performance moyenne sur les classes Cassava. EfficientNetV2M, Accuracy = 0.99. EfficientNetV2M, Macro F1- score = 0.98. EfficientNetV2M, Weighted F1 = 0.99. EfficientNetV2M, Observations clés = Meilleure performance globale, bonne gestion des classes déséquilibrées comme Cassava. ViT16, Accuracy = 0.98. ViT16, Macro F1- score = 0.97. ViT16, Weighted F1 = 0.98. ViT16, Observations clés = Meilleure performance globale, bonne gestion des classes déséquilibrées comme Cassava", "answer_error": ""}
