import os
import pickle
import faiss
import numpy as np
from pathlib import Path
from tqdm import tqdm
import textwrap

from llama_index.readers.file import PDFReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import VectorStoreIndex

# ðŸ“ ParamÃ¨tres
DOCS_DIR = "data"
VECTOR_DIR = "vectordb"
INDEX_FILE = os.path.join(VECTOR_DIR, "index.faiss")
CHUNKS_FILE = os.path.join(VECTOR_DIR, "chunks.pkl")
EMBEDDING_MODEL = "dangvantuan/french-document-embedding"

os.makedirs(VECTOR_DIR, exist_ok=True)

# ðŸ“¥ Chargement manuel des PDF
print("ðŸ“¥ Lecture des fichiers PDF...")
reader = PDFReader()
documents = []

for pdf_path in Path(DOCS_DIR).glob("*.pdf"):
    print(f" - ðŸ“„ {pdf_path.name}")
    docs = reader.load_data(pdf_path)  # âœ… CORRECTION : path au lieu de file
    documents.extend(docs)

print(f"âœ… {len(documents)} documents PDF chargÃ©s.")

# âœ‚ï¸ Chunking par taille de tokens (plus stable que par phrases)
print("âœ‚ï¸ Chunking avec SentenceSplitter (512 tokens, overlap 64)...")
parser = SentenceSplitter(chunk_size=512, chunk_overlap=64)
nodes = parser.get_nodes_from_documents(documents)

print(f"âœ… {len(nodes)} chunks gÃ©nÃ©rÃ©s.")

# ðŸ” AperÃ§u des 5 premiers chunks
print("\nðŸ§© AperÃ§u des 5 premiers chunks :\n")
for i, node in enumerate(nodes[:5]):
    preview = textwrap.shorten(node.get_content().replace("\n", " "), width=200)
    print(f"Chunk {i+1:>2}: {preview}")

# ðŸ”¢ Embedding + FAISS
print("\nðŸ”¢ GÃ©nÃ©ration des embeddings et indexation FAISS...")
embed_model = HuggingFaceEmbedding(model_name=EMBEDDING_MODEL, trust_remote_code=True)
embedding_dim = np.array(embed_model.get_query_embedding("test")).shape[0]
faiss_index = faiss.IndexFlatL2(embedding_dim)
vector_store = FaissVectorStore(faiss_index=faiss_index)

# ðŸ§  Construction de lâ€™index vectoriel
index = VectorStoreIndex(nodes, embed_model=embed_model, vector_store=vector_store)

# ðŸ’¾ Sauvegarde
print("ðŸ’¾ Sauvegarde de lâ€™index et des chunks...")
faiss.write_index(faiss_index, INDEX_FILE)
chunks = [node.get_content() for node in nodes]
with open(CHUNKS_FILE, "wb") as f:
    pickle.dump(chunks, f)

print(f"\nâœ… {len(chunks)} chunks sauvegardÃ©s dans {CHUNKS_FILE}")
